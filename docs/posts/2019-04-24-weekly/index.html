<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="&lt;3">
<title>
Call me Ishmael - the fishman diaries
</title>











<link rel="stylesheet" href="/diaries/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Call me Ishmael"/>
<meta name="twitter:description" content="24 April 2019, weekly newsletter"/>

<meta property="og:title" content="Call me Ishmael" />
<meta property="og:description" content="24 April 2019, weekly newsletter" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://karetsu.github.io/diaries/posts/2019-04-24-weekly/" />
<meta property="article:published_time" content="2019-04-24T09:00:00+00:00" />
<meta property="article:modified_time" content="2019-04-24T09:00:00+00:00" />


    

    
    
    
    <title>
        
        Call me Ishmael
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title">Call me Ishmael</div>

        
<div class="section" id="content">
    Apr 24, 2019 &#183; 404 words
    
    <hr/>
    

<p>Whilst we are waiting for enough content for Huw to get the newsletter together
I’m going to throw around some “interesting things wot I read this week” posts.
I might even make the next one much prettier than this is if people are
interested. All feedback welcome.</p>

<h2 id="opinion">Opinion</h2>

<ul>
<li><a href="https://bentoml.com/posts/2019-04-19-one-model">One model to rule them all</a></li>
<li><a href="https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/">Better
questions</a>:
opinion piece on how we should be thinking about applying data science, a lot
of it is based on Tukey’s 1962 paper The Future of Data Analysis</li>
</ul>

<h2 id="training-development-and-general-learning">Training, development and general learning</h2>

<ul>
<li><a href="https://course.spacy.io/">Advanced topics in NLP</a>: spaCy are hosting a ‘how
to use spaCy for more than basic NLP tasks’ kind of course, it is all on the
website and all free!</li>
<li><a href="https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch">Building Hamiltonian Monte Carlo functions from
scratch</a>:
Colin Carroll posts about how to build a variant of MCMC from scratch (as used
in all of your favourite algorithms). This is part 2 of the series, but part 1
is linked at the top of it!</li>
<li><a href="https://r-graphics.org/">R Graphics Cookbook</a>: not a new book, but the author
has just pushed a 2nd edition of this, and it was already a great resource for
those of us who can never remember all the innards of the R plotting
ecosystem.</li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
<li><a href="https://arxiv.org/pdf/1610.04167.pdf">Tensorial Mixture Models</a>: an attempt
to fix the tractable marginalisation issues of things such as GANs, but in a
suitable computational way (other attempts require additional learning of
circuits which hinders their performance).</li>
<li><a href="http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf">Extracting and Composing Robust Features with De-noising
autoencoders</a>:
(Direct copy-pasta of the abstract) Previous work has shown that the
difficulties in learning deep generative or discriminative models can be
overcome by an initial unsupervised learning step that maps inputs to useful
intermediate representations. We introduce and motivate a new training
principle for unsupervised learning of a representation based on the idea of
making the learned representations robust to partial corruption of the input
pattern. This approach can be used to train autoencoders, and these denoising
autoencoders can be stacked to initialize deep architectures. The algorithm
can be motivated from a manifold learning and information theoretic
perspective or from a generative model perspective. Comparative experiments
clearly show the surprising advantage of corrupting the input of autoencoders
on a pattern classification benchmark suite.</li>
<li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Tran_Missing_Modalities_Imputation_CVPR_2017_paper.pdf">Missing Modalities Imputation via Cascaded Residual
Autoencoder</a>:
does what it says on the tin.</li>
</ul>

<h2 id="origin-of-the-title">Origin of the Title</h2>

<p>I guess if you don&rsquo;t know this one you have a <a href="https://www.amazon.co.uk/Little-Master-Melville-Moby-Dick-BabyLit/dp/1423632044">lot of reading to
do</a>
(｡◕‿‿◕｡).</p>

</div>


        
<div class="section bottom-menu">
    

    
        <a href="/diaries/posts">back</a>
        
    

    
</p>
</div>


        <div class="section footer"></div>
    </div>
</body>

</html>