<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="&lt;3">
<title>
A New Hope - the fishman diaries
</title>











<link rel="stylesheet" href="/diaries/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="A New Hope"/>
<meta name="twitter:description" content="29 November 2019, guest newsletter"/>

<meta property="og:title" content="A New Hope" />
<meta property="og:description" content="29 November 2019, guest newsletter" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://karetsu.github.io/diaries/posts/2019-11-29-weekly/" />
<meta property="article:published_time" content="2019-12-02T09:57:55+00:00" />
<meta property="article:modified_time" content="2019-12-02T09:57:55+00:00" />


    

    
    
    
    <title>
        
        A New Hope
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title">A New Hope</div>

        
<div class="section" id="content">
    Dec 2, 2019 &#183; 753 words
    
    <hr/>
    

<p>Who&rsquo;s more foolish? The fool or the fool who follows him?</p>

<h2 id="post-of-the-week">Post of the week</h2>

<p>I&rsquo;m sure it is no surprise that I&rsquo;m a lover of horror. Literature, films and a
whole bunch of other things. So, what happens if we let GPT-2 generate lots of
horror stories given a little pre-warmup from us? Well <a href="https://theoutline.com/post/8302/ai-incredible-horror-stories?zd=3&amp;zi=hes4n2bb">this
post</a>
does exactly that. Whilst the content of the output is generally trash
(even the recipe) I do love that it gets pretty dark, pretty quick!</p>

<h2 id="blogs-and-commentary">Blogs and commentary</h2>

<ul>
<li>Ever wanted to make a voiceover but don&rsquo;t like how you sound? Now, you can
<a href="https://futurism.com/the-byte/deepfake-ai-look-sound-joe-rogan">sound just like Joe
Rogan</a>
instead.</li>
<li>Artificial Life is a new topic to me, but the folks over at The
Gradient have a <a href="https://thegradient.pub/an-introduction-to-artificial-life-for-people-who-like-ai/">good write
up</a>.</li>
<li>El Reg are back and <a href="https://www.theregister.co.uk/2019/11/26/you_look_like_a_thing_and_i_love_you_review/">promoting a book I
want</a>.
It all links back into the machine learning fatigue I am getting from trying
to explain the limitations of what is possible to those who think its wizardry
(a blog post coming on something similar to this soon).</li>
<li>The last bastion of hope in the world of Go <a href="https://www.theverge.com/2019/11/27/20985260/ai-go-alphago-lee-se-dol-retired-deepmind-defeat">has finally been beaten by
DeepMind</a>.
Turns out computers (note, a huge supercomputer) are good at keeping track of all of the possible trees of
moves and follow on moves to allow them to win within a very strict set of
rules. Who knew?</li>
<li>I&rsquo;m not very good with social media, so if you want to find some good people
in the field to follow then you cannot go wrong with <a href="https://sifted.eu/articles/30-ai-people-in-europe-to-follow-on-twitter/">sifted.eu&rsquo;s
recommendations</a>.</li>
<li>If ever there was a clickbait title its <a href="https://medium.com/@robert.munro/bias-in-ai-3ea569f79d6a">&ldquo;Diversity in AI is not your problem,
it&rsquo;s hers&rdquo;</a>. As you
can probably tell by the fact I&rsquo;m including it here, the title isn&rsquo;t to be
taken literally and the article describes how woefully inept our NLP models
are by recognising &ldquo;his&rdquo; as a pronoun but think that &ldquo;hers&rdquo; is a noun.</li>
<li>MIT Technology Review has a quick summary of what Facebook are up to with
regards to its <a href="https://www.technologyreview.com/f/614774/this-is-how-facebooks-ai-looks-for-bad-stuff/">treatment of video
nasties</a>.
Still the problem of understanding meaning and context of language eludes even
some of the best language modellers.</li>
<li>Over at Google the focus is a little more about explicability of the models
with the &lsquo;launch&rsquo; of <a href="https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf">explainable
AI</a>.
I&rsquo;m really sceptical of this because of the sheer complexity of the inner
workings, but if it proves me wrong I will be the first one celebrating.</li>
<li>Something very pertinent from Heartbeat, <a href="https://heartbeat.fritz.ai/deep-learning-has-a-size-problem-ea601304cd8">deep learning has a size
problem</a>.
I wholeheartedly agree with the sentiment of moving research away from state
of the art accuracy toward state of the art <em>efficiency</em>. I&rsquo;d still prefer
that we didn&rsquo;t have experiments like MegatronLM using up the equivalent of a
single American&rsquo;s annual energy consumption in the (circa) 9 days it took to
train though. Although the idea of pruning weights does make me want to see if
we can get chaos from it.</li>
<li><a href="https://twitter.com/vboykis">Vicki Boykis</a> has a good
<a href="https://vicki.substack.com/p/were-still-in-the-steam-powered-days?utm_campaign=The%20Data%20Science%20Roundup&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">description</a>
of how we really aren&rsquo;t much past the middle ages in terms of how we work with
and apply ML models and outputs.</li>
<li>In attempting to model the ways in which a human brain thinks and learns,
Nature have good <a href="https://www.nature.com/articles/s41586-019-1677-2">overview of the developments in neuromorphic
computing</a>. Warning: this
is behind a paywall and you need your OpenAthens account.</li>
<li>It seems that robots are really here to take our jobs. Cards Against Humanity
has <a href="https://www.techspot.com/news/82977-cards-against-humanity-built-ai-replace-their-writers.html">built an AI to replace their
writers</a></li>
<li>The Gradient doubling up on this list with a description of <a href="https://thegradient.pub/an-epidemic-of-ai-misinformation/">an epidemic of AI
misinformation</a> and
this time is complemented with a post from Forbes about how <a href="https://www.forbes.com/sites/fernandezelizabeth/2019/11/30/ai-is-not-similar-to-human-intelligence-thinking-so-could-be-dangerous/">thinking that AI
is similar to human intelligence could be
dangerous</a>
(note: it is about a joint paper from the Oxford Internet Institute and the Alan
Turing Institute but this is a bit shorter).</li>
</ul>

<h2 id="papers">Papers</h2>

<p>Only one this week, and I think it deserves the award for <a href="https://arxiv.org/abs/1911.11423">best abstract
2019</a>.</p>

<h2 id="haskell-propaganda">Haskell propaganda</h2>

<ul>
<li>Okay, not strictly Haskell propaganda given that you can read half of it
without seeing anything about a computer, but Bogdan Penkovsky&rsquo;s writeup of
<a href="http://penkovsky.com/neural-networks/day5/">convolutional neural networks (with
implementation)</a> is really good.</li>
<li>I&rsquo;ve seen things you people wouldn&rsquo;t believe. <a href="https://dev.to/therewillbecode/modelling-the-world-of-blade-runner-with-haskell-s-type-system-41af">Haskell + Blade Runner = tears in
rain</a>.</li>
<li>Wonder how to combine nix and Haskell? Enjoy <a href="https://www.srid.ca/haskell-nix.html">this
cookbook</a>. I swear by a combination of
nix and cabal for all my haskell needs so this is quite useful.</li>
<li>Does this list of Haskell propaganda contain a link to a list of Haskell
propaganda? <a href="https://williamyaoh.com/posts/2019-11-24-design-and-testing-articles.html">Yes, it
does</a>.</li>
</ul>

<h2 id="origin-of-the-title">Origin of the title</h2>

<p>Star Wars.</p>

</div>


        
<div class="section bottom-menu">
    

    
        <a href="/diaries/posts">back</a>
        
    

    
</p>
</div>


        <div class="section footer"></div>
    </div>
</body>

</html>