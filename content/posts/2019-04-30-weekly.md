---
title: "η-conversion"
date: 2019-04-30T09:00:00Z
draft: false
author: aloysius
image: images/logo.jpg
description: "30 April 2019, weekly newsletter"
---

Time again for me to throw a list of things that have passed by my eyes this
week. For those who are not aware I’ve also included some
links to the recent events at DataCamp where the CEO has gotten away with
sexually assaulting a member of staff and join the course developers in urging
you to boycott this service.


## The DataCamp story

* https://juliasilge.com/blog/datacamp-misconduct
* https://ironholds.org/datacamp
* https://noamross.github.io/datacamp-sexual-assault/

Now, back to our scheduled programming.


## R

* [Ghibli movie colours schemes](https://github.com/ewenme/ghibli): if you want
  your graphs to remind you of some of the best animated films ever made this is
  the package for you. There’s also a website built with a bit more of a
  showcase than the repo allows.


## Python

* [wtfpython](https://github.com/satwikkansal/wtfpython): I think the jump from
  beginner to intermediate level programming in python is when you are genuinely
  able to reason about why something behaves differently to what you would expect.
  This is a very good curated list of such gotchas and can help many people make
  this jump [credit to Arthur Eidukas for this one].
* [vaex](https://towardsdatascience.com/vaex-a-dataframe-with-super-strings-789b92e8d861):
  better string treatment (and much faster than default strings in panda
  dataframes)


## Data quality

There were a couple of things regarding data quality this week so I have given
it its own section.

* [Taming data
  quality](https://quickbooks-engineering.intuit.com/taming-data-quality-with-circuit-breakers-dbe550d3ca78):
  the idea in this one is to use the idea of circuit breakers to help you assess
  the quality of your data. This may also be of interest to anybody in the admin
  data project hub!
* [Data analytics in
  spark](https:/towardsdatascience.com/scalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977):
  large scale data analytics and visualisation using spark


## Blogs of interest

* Andrej Karpathy doesn’t blog very often but when he does they are some of [my
  favourite
  posts](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
  in machine learning. This time he has written [A Recipe for Training Neural
  Networks](https://karpathy.github.io/2019/04/25/recipe/) and I think this is
  pretty fundamental required reading for anybody who intends to perform the
  task of training an NN.
* At times we are probably all guilty of thinking that somebody else’s problem
  is trivial. Chris Done has written a good post about [how to
  avoid](https://chrisdone.com/posts/teaching/) making the people who are
  experiencing these things for the first time feel stupid. Sure, its all common
  sense but its good to be reminded every now and then of how hard things are
  when you experience them for the first time (I think there’s a genuine Dark
  Souls analogy here but I’ll leave that meme well alone).
* [Open questions about GANs](https://distill.pub/2019/gan-open-problems/): a
  blog from a member of the Google Brain Team about questions regarding
  applicability and functionality of GANs which are currently unanswered. I
  quite enjoyed this one as a contrast to the “just use a GAN” type commentary
  that goes on elsewhere. It really shows that we do not understand them very
  well at present.
* [Why programming takes longer than expected
  :D](https://erikbern.com/2019/04/15/why-software-projects-take-longer-than-you-think-a-statistical-model.html):
  a statistical approach to explaining programming project timelines!
* [BERT training times massively
  reduced](https://medium.com/syncedreview/new-google-brain-optimizer-reduces-bert-pre-training-time-from-days-to-minutes-b454e54eda1d):
  Huzzah! Maybe we mere mortals without 4 days of GPU farm time/cost can
  actually train a BERT model


## Papers

* [Intelligible speech synthesis from neural decoding of spoken
  sentences](https://www.biorxiv.org/content/10.1101/481267v1.full): paper on
  decoding speech, not much more to say other than I found it really
  interesting. It might be of use to Theodore and Michaela but I’m not sure how
  finished your work is on the speech stuff.
* [The unreasonable effectiveness of mathematics in the natural
  sciences](https://www.maths.ed.ac.uk/~v1ranick/papers/wigner.pdf): I re-read
  the paper which got riffed on for a machine learning related post
* [Diagnosing and enhancing VAE models](https://arxiv.org/abs/1903.05789):
  understanding the underlying energy function of a variational autoencoder with
  a bit more rigour than the usual treatments


## Origin of the title

This one is pronounced "eta conversion" and comes as one of the important
concepts of the [Lambda
Calculus](http://www.inf.fu-berlin.de/lehre/WS03/alpi/lambda.pdf).


And that is all for now. See you next week, same bat-time, same bat-channel.
