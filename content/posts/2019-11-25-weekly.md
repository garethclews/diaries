---
title: "Party all the time"
date: 2019-11-25T09:00:00Z
draft: false
author: aloysius
image: images/logo.jpg
description: "25 November 2019, guest newsletter"
---

Time is limited this week, so expect a rushed description of the content, but it
is still better than no content!


## Post of the week

I'm not convinced that this isn't because of personal bias toward the current
considerations of the applications of machine learning but I very much enjoyed
reading a presentation from Arvind Narayanan from Princeton University.

Comparing companies selling 'AI' (cf. last week's post) with the [snake oil
salesmen](https://en.wikiquote.org/wiki/Snake_oil) is something I have done for
quite some time now, but Arvind gives some ways to [recognise AI snake
oil](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf).


## Blogs and commentary

- Narrowly missing out on my top post, but also suggested to me by Huw, is a
  medium post on how to deal with doing [data science in the
  wild](https://towardsdatascience.com/data-science-is-boring-1d43473e353e) and
  how to mitigate the humdrum nature of not being a university researcher and
  containing suggestions on how to beat the boredom.
- Technically about marketing, I quite enjoyed [this
  post's](https://marketoonist.com/2019/11/kpi-overload.html) imagery and felt
  that it very well summed up people's obsession with dashboards!
- Wired led with an opinion piece on applications of 'AI for good' and how they
  are often [actually
  bad](https://www.wired.com/story/opinion-ai-for-good-is-often-bad/).
- The [Deep Learning with
  PyTorch](https://pytorch.org/deep-learning-with-pytorch) book has been made
  free. Go grab a copy!
- Uber have generated some [fancy 3D city
  visualisations](https://eng.uber.com/3d-tiles-loadersgl/). They're pretty and
  something pretty cool.


## Papers

I've only read one paper this week. I went back and revisited the original paper
on [variational autoencoders](https://arxiv.org/pdf/1312.6114.pdf) to help me
out with the implementation below.


## Functional programming propaganda
- I quite enjoyed seeing a Haskell, from scratch [implementation of a
  variational
  autoencoder](https://www.declanoller.com/2019/11/15/variational-autoencoders-in-haskell-or-how-i-learned-to-stop-worrying-and-turn-my-friends-into-dogs/).
  So much so that I will be doing something similar myself. I'm not sure if you
  could really pass the pictures of their friends off as genuine dog photos but
  with a latent space of only two dimensions its still really good!
- Michael Snoyman is a prolific writer about all things Haskell. This time he
  has written a [Boring Haskell
  Manifesto](https://www.snoyman.com/blog/2019/11/boring-haskell-manifesto) to
  help people to get Haskell into their organisation.


## Origin of the title

Spotify recommended me a modern cover of this "classic" and it has been in my
head since (for those interested the new one is by a band called Thank You Scientist)
{{< youtube iWa-6g-TbgI >}}

