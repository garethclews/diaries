---
title: "Bad Men"
date: 2020-01-13T05:00:00Z
draft: false
author: aloysius
image: images/logo.jpg
description: "13 January 2020, weekly newsletter"
---


A quiet week to start the year off. Wading through a fair share of 'best of'
collections only one has made it as far as this week's post!




## Post of the Week

It sounds awfully grand, but the *New Renaissance* is an interesting topic in
[an
article](https://www.forbes.com/sites/cognitiveworld/2020/01/05/2020-will-bring-a-new-renaissance-humanity-over-technology/)
by Gerd Leonhard (a very popular futurist). I won't attempt to summarise it here
because its short enough already and I think you should go read it.


## Blogs and commentary

- I do enjoy a good 'best visualisations of...' post and [2019 doesn't let me
  down](https://flowingdata.com/2019/12/19/best-data-visualization-projects-of-2019).
  Here is just one of the examples, made using R.
  {{< figure src="https://www.data-imaginist.com/art/005_genesis/genesis4321.png" title="Generative aRt" width="100%">}}
- Towards Data Science have a pretty good summary of the most important bits of
  maths you need to know from [machine learning which are necessary for deep
  learning](https://towardsdatascience.com/machine-learning-necessary-for-deep-learning-2095a345ec2c)
  including things like Killback-Leibler divergence, point estimators and
  maximum likelihood estimation.
- Of all possible 'mis-wirings' of the human brain I think that Synesthesia is
  the coolest. Being able to 'see' sounds is pretty cool. Now, Jun Wu puts
  forward the case that it [should inspire the next tranche of AI
  research](https://towardsdatascience.com/synesthesia-an-inspiring-condition-for-ai-researchers-10cd57708855).
- I <i class="tf-ion-heart"></i> [gauge
  theory](http://mathworld.wolfram.com/GaugeTheory.html), so adding it into CNNs
  in 'gauge-equivariant CNNs' makes me pretty happy. These extend the ability of
  learning past 2 dimension which may allow [deep learning on curved
  surfaces](https://www.quantamagazine.org/an-idea-from-physics-helps-ai-see-in-higher-dimensions-20200109/)
  to start being a real thing.


## Papers

- Machine learning is getting its groove on even in [compiler
  optimisation](http://proceedings.mlr.press/v97/mendis19a/mendis19a.pdf) these
  days. It doesn't seem to be much fancier than an RNN with LSTM components. I'm
  interested in picking this apart to understand the cases where code becomes
  less optimal but that might be a ways away yet.


## Tools of the trade
Starting off more generically:

- Put the Dask down! Time to try [ray](https://ray.io/).
- fast.ai have published a [data
  questionnaire](https://www.fast.ai/2020/01/07/data-questionnaire/) to help you
  tackle the common problems in data projects.

Two types of propaganda this week!

#### Julia propaganda

- [Julia for R
  programmers](http://pages.stat.wisc.edu/~bates/JuliaForRProgrammers.pdf) may
  only be a presentation but it does a very good job of showcasing the
  similarities and differences between the two. It even has some examples of
  using the Distributions package, interfacing with BLAS and LAPACK (linear
  algebra) libraries and performing a ridge regression.


#### Haskell propaganda

- Category theory has become a really useful for writing elegant and
  maintainable code. MIT have a ['from scratch'
  course](http://brendanfong.com/programmingcats.html) to show you how great
  they are.
- Lenses are ace. Go [find out
  more](https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/a-little-lens-starter-tutorial).



## Origin of the title

> Bad men need nothing more to compass their end, than that good men look on and
> do nothing

-- John Stuart Mill
