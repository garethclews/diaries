---
title: "Call me Ishmael"
date: 2019-04-24T09:00:00Z
author: aloysius
image: images/logo.jpg
description: "24 April 2019, weekly newsletter"
---

Whilst we are waiting for enough content for Huw to get the newsletter together
I’m going to throw around some “interesting things wot I read this week” posts.
I might even make the next one much prettier than this is if people are
interested. All feedback welcome.


## Opinion

* [One model to rule them all](https://bentoml.com/posts/2019-04-19-one-model)
* [Better
  questions](https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions/):
  opinion piece on how we should be thinking about applying data science, a lot
  of it is based on Tukey’s 1962 paper The Future of Data Analysis


## Training, development and general learning

* [Advanced topics in NLP](https://course.spacy.io/): spaCy are hosting a ‘how
  to use spaCy for more than basic NLP tasks’ kind of course, it is all on the
  website and all free!
* [Building Hamiltonian Monte Carlo functions from
  scratch](https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch):
  Colin Carroll posts about how to build a variant of MCMC from scratch (as used
  in all of your favourite algorithms). This is part 2 of the series, but part 1
  is linked at the top of it!
* [R Graphics Cookbook](https://r-graphics.org/): not a new book, but the author
  has just pushed a 2nd edition of this, and it was already a great resource for
  those of us who can never remember all the innards of the R plotting
  ecosystem.


## Papers

* [Tensorial Mixture Models](https://arxiv.org/pdf/1610.04167.pdf): an attempt
  to fix the tractable marginalisation issues of things such as GANs, but in a
  suitable computational way (other attempts require additional learning of
  circuits which hinders their performance).
* [Extracting and Composing Robust Features with De-noising
  autoencoders](http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf):
  (Direct copy-pasta of the abstract) Previous work has shown that the
  difficulties in learning deep generative or discriminative models can be
  overcome by an initial unsupervised learning step that maps inputs to useful
  intermediate representations. We introduce and motivate a new training
  principle for unsupervised learning of a representation based on the idea of
  making the learned representations robust to partial corruption of the input
  pattern. This approach can be used to train autoencoders, and these denoising
  autoencoders can be stacked to initialize deep architectures. The algorithm
  can be motivated from a manifold learning and information theoretic
  perspective or from a generative model perspective. Comparative experiments
  clearly show the surprising advantage of corrupting the input of autoencoders
  on a pattern classification benchmark suite.
* [Missing Modalities Imputation via Cascaded Residual
  Autoencoder](http://openaccess.thecvf.com/content_cvpr_2017/papers/Tran_Missing_Modalities_Imputation_CVPR_2017_paper.pdf):
  does what it says on the tin.


## Origin of the Title

I guess if you don't know this one you have a [lot of reading to
do](https://www.amazon.co.uk/Little-Master-Melville-Moby-Dick-BabyLit/dp/1423632044)
(｡◕‿‿◕｡).
